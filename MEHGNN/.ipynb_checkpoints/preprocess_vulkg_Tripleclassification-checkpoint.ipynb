{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977298e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4fdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix = 'data/preprocessed/VULKG_processed/'\n",
    "num_ntypes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c7c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data, delete movies with no actor or director\n",
    "df = pd.read_excel('data/raw/VULKG/label_with_negatives.xlsx')[:].dropna(\n",
    "    axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1096494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels, and delete movies with unwanted genres\n",
    "# 0 for action, 1 for comedy, 2 for drama, -1 for others\n",
    "labels = df['label'].values\n",
    "# for movie_idx, genres in movies['genres'].iteritems():\n",
    "#     labels[movie_idx] = -1\n",
    "#     for genre in genres.split('|'):\n",
    "#         if genre == 'Action':\n",
    "#             labels[movie_idx] = 0\n",
    "#             break\n",
    "#         elif genre == 'Comedy':\n",
    "#             labels[movie_idx] = 1\n",
    "#             break\n",
    "#         elif genre == 'Drama':\n",
    "#             labels[movie_idx] = 2\n",
    "#             break\n",
    "# unwanted_idx = np.where(labels == -1)[0]\n",
    "# movies = movies.drop(unwanted_idx).reset_index(drop=True)\n",
    "# labels = np.delete(labels, unwanted_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d880e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cves = list(set(df['cveID']))\n",
    "cves.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf96dca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(cves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077f3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvs = list(set(df['Product Version']))\n",
    "pvs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369e185a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "print(len(pvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c916c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the adjacency matrix for the graph consisting of cves, pvs\n",
    "# 0 for cves, 1 for pvs\n",
    "dim = len(cves) + len(pvs)\n",
    "type_mask = np.zeros((dim), dtype=int)\n",
    "type_mask[len(cves):] = 1\n",
    "\n",
    "adjM = np.zeros((dim, dim), dtype=int)\n",
    "for i in range(len(df)):\n",
    "    cve = df['cveID'].iloc[i]\n",
    "    pv = df['Product Version'].iloc[i]\n",
    "    cve_idx = cves.index(cve)\n",
    "    pv_idx = pvs.index(pv)\n",
    "    adjM[cve_idx, len(cves)+pv_idx] = 1\n",
    "    adjM[len(cves)+pv_idx, cve_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e725472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "# 矩阵的尺寸\n",
    "rows, cols = len(cves), 100\n",
    "\n",
    "# 稀疏度：非零元素的比例\n",
    "density = 0.05  # 例如，5%的元素是非零的\n",
    "\n",
    "# 使用scipy.sparse中的random函数生成随机稀疏矩阵\n",
    "# 数据默认服从[0, 1)的均匀分布\n",
    "cve_X = sparse_random(rows, cols, density, format='csr')\n",
    "adjM_da2m = adjM[len(cves):, :len(cves)]\n",
    "adjM_da2m_normalized = np.diag(1 / adjM_da2m.sum(axis=1)).dot(adjM_da2m)\n",
    "director_actor_X = scipy.sparse.csr_matrix(adjM_da2m_normalized).dot(cve_X)\n",
    "full_X = scipy.sparse.vstack([cve_X, director_actor_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5513458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 152)\n",
      "(152, 100)\n",
      "(353, 100)\n"
     ]
    }
   ],
   "source": [
    "print(adjM_da2m_normalized.shape)\n",
    "print(cve_X.shape)\n",
    "print(full_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a82970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_metapaths = [\n",
    "    [(0, 1, 0)],\n",
    "    [(1, 0, 1)]\n",
    "]\n",
    "# create the directories if they do not exist\n",
    "for i in range(num_ntypes):\n",
    "    pathlib.Path(save_prefix + '{}'.format(i)).mkdir(parents=True, exist_ok=True)\n",
    "for i in range(num_ntypes):\n",
    "    # get metapath based neighbor pairs\n",
    "    neighbor_pairs = utils.preprocess.get_metapath_neighbor_pairs(adjM, type_mask, expected_metapaths[i])\n",
    "    # construct and save metapath-based networks\n",
    "    #print(neighbor_pairs[0][(0,0)])\n",
    "    G_list = utils.preprocess.get_networkx_graph(neighbor_pairs, type_mask, i)\n",
    "    #print(G_list[0].nodes())\n",
    "    # networkx graph (metapath specific)\n",
    "    for G, metapath in zip(G_list, expected_metapaths[i]):\n",
    "        nx.write_adjlist(G, save_prefix + '{}/'.format(i) + '-'.join(map(str, metapath)) + '.adjlist')\n",
    "    # node indices of edge metapaths\n",
    "    all_edge_metapath_idx_array = utils.preprocess.get_edge_metapath_idx_array(neighbor_pairs)\n",
    "    for metapath, edge_metapath_idx_array in zip(expected_metapaths[i], all_edge_metapath_idx_array):\n",
    "        np.save(save_prefix + '{}/'.format(i) + '-'.join(map(str, metapath)) + '_idx.npy', edge_metapath_idx_array)\n",
    "\n",
    "# save data\n",
    "# all nodes adjacency matrix\n",
    "scipy.sparse.save_npz(save_prefix + 'adjM.npz', scipy.sparse.csr_matrix(adjM))\n",
    "# all nodes (movies, directors and actors) features\n",
    "for i in range(num_ntypes):\n",
    "    scipy.sparse.save_npz(save_prefix + 'features_{}.npz'.format(i), full_X[np.where(type_mask == i)[0]])\n",
    "# all nodes (movies, directors and actors) type labels\n",
    "np.save(save_prefix + 'node_types.npy', type_mask)\n",
    "# movie genre labels\n",
    "np.save(save_prefix + 'labels.npy', labels)\n",
    "# movie train/validation/test splits\n",
    "rand_seed = 1566911444\n",
    "train_idx, val_idx = train_test_split(np.arange(len(labels)), test_size=100, random_state=rand_seed)\n",
    "train_idx, test_idx = train_test_split(train_idx, test_size=100, random_state=rand_seed)\n",
    "train_idx.sort()\n",
    "val_idx.sort()\n",
    "test_idx.sort()\n",
    "np.savez(save_prefix + 'train_val_test_idx.npz',\n",
    "         val_idx=val_idx,\n",
    "         train_idx=train_idx,\n",
    "         test_idx=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd31d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_prefix + 'labels.npy', labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
